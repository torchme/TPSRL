 reward2: -481986.5
 reward2: -217141.9
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 95.16666666666667
max episode reward: -31228198.700000018
mean episode reward: -65237242.43333334
min episode reward: -142402976.69999954
total episodes: 42
distance: 699128.4
2022-06-19 15:40:06,761	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6523724.0x the scale of `vf_clip_param`. This means that it will take more than 6523724.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -481986.5
 reward2: -112286.3
 reward2: -145495.2
 reward2: -421004.1
 reward2: -134520.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 95.17857142857143
max episode reward: -30307274.50000002
mean episode reward: -65198009.7952381
min episode reward: -160158795.5999994
total episodes: 84
distance: 1295292.2000000002
2022-06-19 15:40:16,115	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6519801.0x the scale of `vf_clip_param`. This means that it will take more than 6519801.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -481986.5
 reward2: -112286.3
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 96.13
max episode reward: -30307274.50000002
mean episode reward: -66021880.79099999
min episode reward: -162386017.49999937
total episodes: 126
distance: 594272.8
2022-06-19 15:40:25,374	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6602188.0x the scale of `vf_clip_param`. This means that it will take more than 6602188.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -481986.5
 reward2: -313468.0
 reward2: -84755.5
 reward2: -224216.6
 reward2: -228151.1
 reward2: -566499.2
 reward2: -134520.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 94.45
max episode reward: -30009349.700000018
mean episode reward: -64793091.21599999
min episode reward: -162386017.49999937
total episodes: 169
distance: 2662915.2
2022-06-19 15:40:34,553	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6479309.0x the scale of `vf_clip_param`. This means that it will take more than 6479309.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -481986.5
 reward2: -313468.0
 reward2: -84755.5
 reward2: -224216.6
 reward2: -228151.1
 reward2: -566499.2
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 91.36
max episode reward: -25756114.70000001
mean episode reward: -62275589.13800002
min episode reward: -127531546.69999963
total episodes: 214
distance: 2662857.9000000004
2022-06-19 15:40:44,231	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6227559.0x the scale of `vf_clip_param`. This means that it will take more than 6227559.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -481986.5
 reward2: -313468.0
 reward2: -84755.5
 reward2: -224216.6
 reward2: -228151.1
 reward2: -566499.2
 reward2: -179694.6
 reward2: -303735.0
 reward2: -104729.1
 reward2: -235605.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 94.26
max episode reward: -25756114.70000001
mean episode reward: -64559598.741000004
min episode reward: -181223430.59999925
total episodes: 254
distance: 3171235.9000000004
2022-06-19 15:40:53,570	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6455960.0x the scale of `vf_clip_param`. This means that it will take more than 6455960.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -481986.5
 reward2: -112286.3
 reward2: -431877.5
 reward2: -629318.2
 reward2: -747612.3
 reward2: -17192.2
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 89.56
max episode reward: -25756114.70000001
mean episode reward: -60675851.446000025
min episode reward: -181223430.59999925
total episodes: 301
distance: 3149356.2
2022-06-19 15:41:03,099	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6067585.0x the scale of `vf_clip_param`. This means that it will take more than 6067585.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -481986.5
 reward2: -112286.3
 reward2: -431877.5
 reward2: -629318.2
 reward2: -747612.3
 reward2: -178456.6
 reward2: -163976.8
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 84.75
max episode reward: -28469269.900000006
mean episode reward: -56868471.54400002
min episode reward: -107377275.5999998
total episodes: 349
distance: 3478004.8999999994
2022-06-19 15:41:12,292	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 5686847.0x the scale of `vf_clip_param`. This means that it will take more than 5686847.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -474314.1
 reward2: -273596.1
 reward2: -16632.9
 reward2: -244895.6
 reward2: -335644.7
 reward2: -419779.6
 reward2: -718772.7
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 81.79
max episode reward: -28469269.900000006
mean episode reward: -54522467.52700002
min episode reward: -138513869.69999957
total episodes: 399
distance: 3914600.4000000004
2022-06-19 15:41:21,605	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 5452247.0x the scale of `vf_clip_param`. This means that it will take more than 5452247.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -474314.1
 reward2: -54005.0
 reward2: -110244.5
 reward2: -618520.1
 reward2: -757357.4
 reward2: -3030.2
 reward2: -756835.0
 reward2: -315206.8
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 74.02
max episode reward: -24043218.80000001
mean episode reward: -48263844.34200003
min episode reward: -138513869.69999957
total episodes: 457
distance: 4196481.399999999
2022-06-19 15:41:30,822	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4826384.0x the scale of `vf_clip_param`. This means that it will take more than 4826384.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -474314.1
 reward2: -273596.1
 reward2: -16632.9
 reward2: -244895.6
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 71.43
max episode reward: -15609611.299999999
mean episode reward: -46210860.22200003
min episode reward: -99824594.59999985
total episodes: 512
distance: 1802451.4
2022-06-19 15:41:39,961	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4621086.0x the scale of `vf_clip_param`. This means that it will take more than 4621086.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -474314.1
 reward2: -174142.1
 reward2: -327372.0
 reward2: -166037.4
 reward2: -244473.9
 reward2: -219845.3
 reward2: -16632.9
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 72.5
max episode reward: -15609611.299999999
mean episode reward: -47076874.75800004
min episode reward: -107586253.29999979
total episodes: 568
distance: 2660725.9999999995
2022-06-19 15:41:51,472	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4707687.0x the scale of `vf_clip_param`. This means that it will take more than 4707687.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -768051.4
 reward2: -177787.5
 reward2: -16632.9
 reward2: -244895.6
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 68.94
max episode reward: -18797651.1
mean episode reward: -44313924.171000026
min episode reward: -141079332.99999955
total episodes: 627
distance: 2000380.1
2022-06-19 15:42:02,213	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4431392.0x the scale of `vf_clip_param`. This means that it will take more than 4431392.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -768051.4
 reward2: -177787.5
 reward2: -16632.9
 reward2: -244895.6
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 66.93
max episode reward: -15972347.799999997
mean episode reward: -42624938.59800003
min episode reward: -141079332.99999955
total episodes: 687
distance: 2000380.1
2022-06-19 15:42:13,452	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4262494.0x the scale of `vf_clip_param`. This means that it will take more than 4262494.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -768051.4
 reward2: -244473.9
 reward2: -127597.2
 reward2: -335644.7
 reward2: -86059.4
 reward2: -235605.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 61.49
max episode reward: -17194551.099999994
mean episode reward: -38254512.36500003
min episode reward: -126558358.99999966
total episodes: 755
distance: 2764586.5
2022-06-19 15:42:24,241	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3825451.0x the scale of `vf_clip_param`. This means that it will take more than 3825451.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -236277.0
 reward2: -175806.0
 reward2: -199721.1
 reward2: -618520.1
 reward2: -487.7
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 59.11
max episode reward: -17819331.299999997
mean episode reward: -36157532.30700002
min episode reward: -71906376.10000007
total episodes: 824
distance: 1778828.7
2022-06-19 15:42:35,433	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3615753.0x the scale of `vf_clip_param`. This means that it will take more than 3615753.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -768051.4
 reward2: -177787.5
 reward2: -16632.9
 reward2: -244895.6
 reward2: -622239.4
 reward2: -744429.8
 reward2: -166362.4
 reward2: -453956.8
 reward2: -171963.4
 reward2: -431877.5
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 56.53
max episode reward: -12831839.6
mean episode reward: -34196294.18500003
min episode reward: -59510936.50000007
total episodes: 895
distance: 5220527.6
2022-06-19 15:42:46,128	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3419629.0x the scale of `vf_clip_param`. This means that it will take more than 3419629.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -768051.4
 reward2: -177787.5
 reward2: -16632.9
 reward2: -457364.3
 reward2: -3742.2
 reward2: -283543.4
 reward2: -84207.0
 reward2: -174142.1
 reward2: -630921.6
 reward2: -271985.0
 reward2: -276431.2
 reward2: -755221.1
 reward2: -5528.0
 reward2: -424209.6
 reward2: -104729.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 52.16
max episode reward: -14286334.799999997
mean episode reward: -30864909.110000018
min episode reward: -56767762.00000006
total episodes: 973
distance: 5547023.199999999
2022-06-19 15:42:57,746	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3086491.0x the scale of `vf_clip_param`. This means that it will take more than 3086491.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -768051.4
 reward2: -177787.5
 reward2: -16632.9
 reward2: -244895.6
 reward2: -622239.4
 reward2: -744429.8
 reward2: -456119.3
 reward2: -112286.3
 reward2: -71589.3
 reward2: -371363.8
 reward2: -253859.1
 reward2: -490747.6
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 51.04
max episode reward: -15669432.999999996
mean episode reward: -29850536.85500002
min episode reward: -58359799.00000007
total episodes: 1050
distance: 5862364.199999998
2022-06-19 15:43:08,648	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2985054.0x the scale of `vf_clip_param`. This means that it will take more than 2985054.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -768051.4
 reward2: -244473.9
 reward2: -110244.5
 reward2: -15096.1
 reward2: -229143.7
 reward2: -175251.1
 reward2: -162997.9
 reward2: -572434.6
 reward2: -342404.4
 reward2: -58407.9
 reward2: -84879.7
 reward2: -723335.2
 reward2: -457364.3
 reward2: -3742.2
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 49.01
max episode reward: -14023121.199999992
mean episode reward: -28273171.18000001
min episode reward: -58359799.00000007
total episodes: 1134
distance: 5090587.500000001
2022-06-19 15:43:19,738	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2827317.0x the scale of `vf_clip_param`. This means that it will take more than 2827317.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -424209.6
 reward2: -104729.1
 reward2: -319514.1
 reward2: -189389.2
 reward2: -16632.9
 reward2: -722180.2
 reward2: -637952.0
 reward2: -335644.7
 reward2: -112286.3
 reward2: -431877.5
 reward2: -629318.2
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 46.29
max episode reward: -13126648.199999996
mean episode reward: -26250289.742000006
min episode reward: -43968641.40000005
total episodes: 1220
distance: 4530667.0
2022-06-19 15:43:30,945	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2625029.0x the scale of `vf_clip_param`. This means that it will take more than 2625029.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -184388.0
 reward2: -335644.7
 reward2: -419779.6
 reward2: -453956.8
 reward2: -171963.4
 reward2: -569704.7
 reward2: -424209.6
 reward2: -104729.1
 reward2: -319514.1
 reward2: -84207.0
 reward2: -273596.1
 reward2: -16632.9
 reward2: -722180.2
 reward2: -176588.4
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 44.09
max episode reward: -12174168.4
mean episode reward: -24357284.92700001
min episode reward: -44852362.50000004
total episodes: 1312
distance: 4583152.9
2022-06-19 15:43:41,638	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2435728.0x the scale of `vf_clip_param`. This means that it will take more than 2435728.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -217100.4
 reward2: -453956.8
 reward2: -347172.7
 reward2: -142472.0
 reward2: -736918.3
 reward2: -16632.9
 reward2: -584130.6
 reward2: -177157.2
 reward2: -86059.4
 reward2: -346092.0
 reward2: -474314.1
 reward2: -316178.2
 reward2: -611394.1
 reward2: -244473.9
 reward2: -501938.3
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 40.74
max episode reward: -10444329.099999998
mean episode reward: -21687963.894
min episode reward: -35956193.100000024
total episodes: 1410
distance: 5495180.2
2022-06-19 15:43:52,641	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2168796.0x the scale of `vf_clip_param`. This means that it will take more than 2168796.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -184388.0
 reward2: -335644.7
 reward2: -419779.6
 reward2: -453956.8
 reward2: -347172.7
 reward2: -271985.0
 reward2: -71589.3
 reward2: -216560.8
 reward2: -69743.6
 reward2: -183597.9
 reward2: -206262.2
 reward2: -259962.0
 reward2: -294616.7
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 40.39
max episode reward: -11546450.800000003
mean episode reward: -21215459.929
min episode reward: -40975865.50000004
total episodes: 1509
distance: 3479236.1
2022-06-19 15:44:04,177	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2121546.0x the scale of `vf_clip_param`. This means that it will take more than 2121546.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -184388.0
 reward2: -630921.6
 reward2: -165896.5
 reward2: -768051.4
 reward2: -244473.9
 reward2: -244866.3
 reward2: -739349.5
 reward2: -606928.3
 reward2: -318617.8
 reward2: -154626.7
 reward2: -216560.8
 reward2: -455979.1
 reward2: -166618.9
 reward2: -424209.6
 reward2: -104729.1
 reward2: -319514.1
 reward2: -357746.8
 reward2: -548684.1
 reward2: -16632.9
 reward2: -722180.2
 reward2: -176588.4
 reward2: -326058.3
 reward2: -197700.4
 reward2: -352123.8
 reward2: -620111.7
mean episode length: 38.980582524271846
max episode reward: -11084216.399999999
mean episode reward: -20129188.860194176
min episode reward: -48247847.60000005
total episodes: 1612
distance: 9202771.7
2022-06-19 15:44:15,598	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2012919.0x the scale of `vf_clip_param`. This means that it will take more than 2012919.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -229143.7
 reward2: -16632.9
 reward2: -421464.6
 reward2: -419779.6
 reward2: -572434.6
 reward2: -187872.0
 reward2: -224739.4
 reward2: -142959.7
 reward2: -176588.4
 reward2: -326058.3
 reward2: -197700.4
 reward2: -3742.2
 reward2: -373527.6
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 38.25961538461539
max episode reward: -10663899.6
mean episode reward: -19620300.26442308
min episode reward: -39654562.70000004
total episodes: 1716
distance: 3612955.7
2022-06-19 15:44:26,247	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1962030.0x the scale of `vf_clip_param`. This means that it will take more than 1962030.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -229143.7
 reward2: -17192.2
 reward2: -490876.5
 reward2: -224739.4
 reward2: -132673.9
 reward2: -326058.3
 reward2: -316178.2
 reward2: -611394.1
 reward2: -244473.9
 reward2: -253859.1
 reward2: -490747.6
 reward2: -5528.0
 reward2: -424209.6
 reward2: -389605.8
 reward2: -421464.6
 reward2: -313955.6
 reward2: -548016.8
 reward2: -532304.2
 reward2: -224216.6
 reward2: -228151.1
 reward2: -449230.1
 reward2: -635872.7
 reward2: -351352.7
mean episode length: 36.00892857142857
max episode reward: -9464865.700000001
mean episode reward: -17692280.20357143
min episode reward: -39926749.400000036
total episodes: 1828
distance: 8532895.399999999
2022-06-19 15:44:37,784	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1769228.0x the scale of `vf_clip_param`. This means that it will take more than 1769228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -424209.6
 reward2: -104729.1
 reward2: -409449.2
 reward2: -335644.7
 reward2: -313955.6
 reward2: -548016.8
 reward2: -84207.0
 reward2: -197700.4
 reward2: -347172.7
 reward2: -271985.0
 reward2: -138381.5
 reward2: -367492.7
 reward2: -630805.6
 reward2: -15096.1
 reward2: -229143.7
 reward2: -16632.9
 reward2: -459980.0
 reward2: -455979.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 35.01754385964912
max episode reward: -10250881.600000001
mean episode reward: -16920567.578070175
min episode reward: -33305460.40000002
total episodes: 1942
distance: 6171787.2
2022-06-19 15:44:49,110	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1692057.0x the scale of `vf_clip_param`. This means that it will take more than 1692057.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -424209.6
 reward2: -104729.1
 reward2: -319514.1
 reward2: -154959.4
 reward2: -335644.7
 reward2: -313955.6
 reward2: -176588.4
 reward2: -326058.3
 reward2: -197700.4
 reward2: -353550.7
 reward2: -15096.1
 reward2: -217100.4
 reward2: -166037.4
 reward2: -177787.5
 reward2: -547246.6
 reward2: -174290.0
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 33.680672268907564
max episode reward: -9773755.799999997
mean episode reward: -15759370.14117647
min episode reward: -31936285.800000023
total episodes: 2061
distance: 4390444.5
2022-06-19 15:45:00,618	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1575937.0x the scale of `vf_clip_param`. This means that it will take more than 1575937.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -229143.7
 reward2: -16632.9
 reward2: -3447.4
 reward2: -166037.4
 reward2: -244473.9
 reward2: -369721.2
 reward2: -216560.8
 reward2: -490747.6
 reward2: -5528.0
 reward2: -456119.3
 reward2: -112286.3
 reward2: -145495.2
 reward2: -104729.1
 reward2: -229690.5
 reward2: -84755.5
 reward2: -534020.0
 reward2: -84207.0
 reward2: -197700.4
 reward2: -370903.3
 reward2: -630921.6
 reward2: -132673.9
 reward2: -367492.7
 reward2: -504500.4
mean episode length: 32.17741935483871
max episode reward: -8226491.300000001
mean episode reward: -14528550.82983871
min episode reward: -25366890.200000014
total episodes: 2185
distance: 6636487.5
2022-06-19 15:45:13,373	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1452855.0x the scale of `vf_clip_param`. This means that it will take more than 1452855.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -266701.4
 reward2: -127597.2
 reward2: -335644.7
 reward2: -154626.7
 reward2: -216560.8
 reward2: -174538.7
 reward2: -569704.7
 reward2: -424209.6
 reward2: -389605.8
 reward2: -708059.2
 reward2: -224216.6
 reward2: -765427.3
 reward2: -134520.1
 reward2: -15096.1
 reward2: -217100.4
 reward2: -166037.4
 reward2: -177787.5
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 31.519685039370078
max episode reward: -8088919.000000001
mean episode reward: -13823256.472440945
min episode reward: -23413355.700000007
total episodes: 2312
distance: 5084067.100000001
2022-06-19 15:45:24,209	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1382326.0x the scale of `vf_clip_param`. This means that it will take more than 1382326.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -184388.0
 reward2: -335644.7
 reward2: -313955.6
 reward2: -505542.3
 reward2: -166002.7
 reward2: -458706.6
 reward2: -197700.4
 reward2: -67168.3
 reward2: -104729.1
 reward2: -289312.1
 reward2: -110244.5
 reward2: -15096.1
 reward2: -217100.4
 reward2: -572434.6
 reward2: -216560.8
 reward2: -490747.6
 reward2: -5528.0
 reward2: -178405.7
 reward2: -177875.1
 reward2: -742095.8
 reward2: -84879.7
 reward2: -176100.7
 reward2: -139336.7
 reward2: -535101.0
 reward2: -175755.0
mean episode length: 30.549618320610687
max episode reward: -8351079.100000001
mean episode reward: -13070596.27328244
min episode reward: -22502226.300000004
total episodes: 2443
distance: 6696784.5
2022-06-19 15:45:36,017	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1307060.0x the scale of `vf_clip_param`. This means that it will take more than 1307060.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -793012.7
 reward2: -166002.7
 reward2: -58407.9
 reward2: -117026.3
 reward2: -326058.3
 reward2: -197700.4
 reward2: -353876.4
 reward2: -152341.9
 reward2: -244473.9
 reward2: -244866.3
 reward2: -5528.0
 reward2: -756347.3
 reward2: -348406.0
 reward2: -154471.1
 reward2: -86059.4
 reward2: -105714.3
 reward2: -332625.4
 reward2: -620599.4
 reward2: -214105.0
 reward2: -17192.2
 reward2: -530094.3
 reward2: -174290.0
 reward2: -283543.4
 reward2: -175806.0
 reward2: -244895.6
mean episode length: 29.57037037037037
max episode reward: -7066313.6
mean episode reward: -12257338.978518518
min episode reward: -18936900.9
total episodes: 2578
distance: 6887832.2
2022-06-19 15:45:47,554	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1225734.0x the scale of `vf_clip_param`. This means that it will take more than 1225734.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -217100.4
 reward2: -490876.5
 reward2: -229690.5
 reward2: -176100.7
 reward2: -326058.3
 reward2: -197700.4
 reward2: -347172.7
 reward2: -542615.5
 reward2: -30424.4
 reward2: -127597.2
 reward2: -182477.6
 reward2: -3030.2
 reward2: -424209.6
 reward2: -421004.1
 reward2: -739349.5
 reward2: -224216.6
 reward2: -311128.3
 reward2: -313955.6
 reward2: -348893.7
 reward2: -590955.0
 reward2: -16632.9
 reward2: -533501.8
 reward2: -174290.0
mean episode length: 29.514705882352942
max episode reward: -7810274.200000002
mean episode reward: -12211362.753676469
min episode reward: -27848748.80000002
total episodes: 2714
distance: 7643207.4
2022-06-19 15:45:59,304	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1221136.0x the scale of `vf_clip_param`. This means that it will take more than 1221136.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -217100.4
 reward2: -490876.5
 reward2: -229690.5
 reward2: -176100.7
 reward2: -326058.3
 reward2: -197700.4
 reward2: -347172.7
 reward2: -542615.5
 reward2: -30424.4
 reward2: -127597.2
 reward2: -182477.6
 reward2: -177787.5
 reward2: -175251.1
 reward2: -5528.0
 reward2: -424209.6
 reward2: -332625.4
 reward2: -315206.8
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 28.820143884892087
max episode reward: -7133610.4
mean episode reward: -11527613.84532374
min episode reward: -19928494.900000002
total episodes: 2853
distance: 4538791.600000001
2022-06-19 15:46:11,755	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1152761.0x the scale of `vf_clip_param`. This means that it will take more than 1152761.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -217100.4
 reward2: -490876.5
 reward2: -229690.5
 reward2: -176100.7
 reward2: -117076.7
 reward2: -224216.6
 reward2: -166002.7
 reward2: -193296.6
 reward2: -213945.2
 reward2: -3742.2
 reward2: -253341.4
 reward2: -247490.3
 reward2: -177787.5
 reward2: -175251.1
 reward2: -5528.0
 reward2: -424209.6
 reward2: -31909.8
 reward2: -112286.3
 reward2: -275386.0
 reward2: -84207.0
 reward2: -175806.0
 reward2: -722180.2
 reward2: -637952.0
mean episode length: 28.30496453900709
max episode reward: -6278921.6
mean episode reward: -11028233.359574467
min episode reward: -16211632.199999997
total episodes: 2994
distance: 5694080.899999999
2022-06-19 15:46:23,481	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1102823.0x the scale of `vf_clip_param`. This means that it will take more than 1102823.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -217100.4
 reward2: -572434.6
 reward2: -590955.0
 reward2: -16632.9
 reward2: -421464.6
 reward2: -86059.4
 reward2: -125953.2
 reward2: -174538.7
 reward2: -449230.1
 reward2: -183059.1
 reward2: -275410.9
 reward2: -84207.0
 reward2: -197700.4
 reward2: -352611.5
 reward2: -176588.4
 reward2: -176064.6
 reward2: -142461.7
 reward2: -58407.9
 reward2: -224216.6
 reward2: -768051.4
 reward2: -244473.9
 reward2: -183433.9
 reward2: -421004.1
mean episode length: 27.73611111111111
max episode reward: -6041990.400000001
mean episode reward: -10596850.150694443
min episode reward: -17333843.699999996
total episodes: 3138
distance: 6395438.300000002
2022-06-19 15:46:35,280	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1059685.0x the scale of `vf_clip_param`. This means that it will take more than 1059685.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -450076.8
 reward2: -389605.8
 reward2: -259962.0
 reward2: -84207.0
 reward2: -154959.4
 reward2: -45557.6
 reward2: -217100.4
 reward2: -166037.4
 reward2: -244473.9
 reward2: -110244.5
 reward2: -480958.1
 reward2: -617171.4
 reward2: -5528.0
 reward2: -456119.3
 reward2: -86059.4
 reward2: -229690.5
 reward2: -348406.0
 reward2: -333722.1
 reward2: -224216.6
 reward2: -166002.7
 reward2: -732004.7
 reward2: -471109.2
 reward2: -171963.4
 reward2: -174290.0
 reward2: -355186.7
mean episode length: 27.63448275862069
max episode reward: -5555545.2
mean episode reward: -10490203.782068966
min episode reward: -17357544.299999993
total episodes: 3283
distance: 7759042.7
2022-06-19 15:46:46,895	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1049020.0x the scale of `vf_clip_param`. This means that it will take more than 1049020.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -184388.0
 reward2: -183059.1
 reward2: -166362.4
 reward2: -386147.3
 reward2: -104729.1
 reward2: -86704.0
 reward2: -313955.6
 reward2: -176588.4
 reward2: -326058.3
 reward2: -84207.0
 reward2: -532304.2
 reward2: -224216.6
 reward2: -768051.4
 reward2: -244473.9
 reward2: -110244.5
 reward2: -15096.1
 reward2: -618845.8
 reward2: -348406.0
 reward2: -590955.0
 reward2: -16632.9
 reward2: -457364.3
 reward2: -347172.7
 reward2: -271985.0
 reward2: -174290.0
 reward2: -490747.6
mean episode length: 27.183673469387756
max episode reward: -6176629.6
mean episode reward: -9817864.697278911
min episode reward: -13365831.199999997
total episodes: 3430
distance: 7322053.600000001
2022-06-19 15:46:58,211	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 981786.0x the scale of `vf_clip_param`. This means that it will take more than 981786.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -184388.0
 reward2: -327372.0
 reward2: -166037.4
 reward2: -244473.9
 reward2: -183433.9
 reward2: -104729.1
 reward2: -229690.5
 reward2: -176100.7
 reward2: -117076.7
 reward2: -224216.6
 reward2: -166002.7
 reward2: -193296.6
 reward2: -590955.0
 reward2: -16632.9
 reward2: -214759.9
 reward2: -15096.1
 reward2: -354191.8
 reward2: -3742.2
 reward2: -490747.6
 reward2: -5528.0
 reward2: -456119.3
 reward2: -313955.6
 reward2: -277386.3
 reward2: -359295.0
 reward2: -84207.0
mean episode length: 26.899328859060404
max episode reward: -6120694.5
mean episode reward: -9563146.338255035
min episode reward: -14028077.300000003
total episodes: 3579
distance: 5820014.7
2022-06-19 15:47:10,038	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 956315.0x the scale of `vf_clip_param`. This means that it will take more than 956315.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -139213.5
 reward2: -15096.1
 reward2: -217100.4
 reward2: -166037.4
 reward2: -177787.5
 reward2: -16632.9
 reward2: -721692.6
 reward2: -176100.7
 reward2: -139336.7
 reward2: -71589.3
 reward2: -611975.6
 reward2: -492019.1
 reward2: -3742.2
 reward2: -283543.4
 reward2: -245709.6
 reward2: -313955.6
 reward2: -334217.0
 reward2: -104729.1
 reward2: -235605.1
 reward2: -54005.0
 reward2: -127597.2
 reward2: -179853.6
 reward2: -739349.5
 reward2: -224216.6
 reward2: -166002.7
mean episode length: 26.812080536912752
max episode reward: -4935248.5
mean episode reward: -9354348.351006709
min episode reward: -13728263.799999999
total episodes: 3728
distance: 6736096.899999999
2022-06-19 15:47:21,836	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 935435.0x the scale of `vf_clip_param`. This means that it will take more than 935435.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -152923.4
 reward2: -152341.9
 reward2: -274792.8
 reward2: -545937.5
 reward2: -736918.3
 reward2: -16632.9
 reward2: -533501.8
 reward2: -431877.5
 reward2: -354191.8
 reward2: -3742.2
 reward2: -490747.6
 reward2: -420845.1
 reward2: -31909.8
 reward2: -217141.9
 reward2: -53841.1
 reward2: -316178.2
 reward2: -574001.5
 reward2: -718772.7
 reward2: -176588.4
 reward2: -90453.1
 reward2: -346092.0
 reward2: -166002.7
 reward2: -58407.9
 reward2: -623955.2
mean episode length: 26.490066225165563
max episode reward: -5735013.6
mean episode reward: -9067764.052317882
min episode reward: -15370973.399999997
total episodes: 3879
distance: 7774618.100000001
2022-06-19 15:47:33,534	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 906776.0x the scale of `vf_clip_param`. This means that it will take more than 906776.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -152923.4
 reward2: -152341.9
 reward2: -274792.8
 reward2: -84207.0
 reward2: -197700.4
 reward2: -470357.3
 reward2: -17192.2
 reward2: -530094.3
 reward2: -535101.0
 reward2: -721692.6
 reward2: -142461.7
 reward2: -58407.9
 reward2: -224216.6
 reward2: -630805.6
 reward2: -480958.1
 reward2: -90453.1
 reward2: -86704.0
 reward2: -313955.6
 reward2: -348893.7
 reward2: -216560.8
 reward2: -253341.4
 reward2: -127597.2
 reward2: -303735.0
 reward2: -421004.1
mean episode length: 26.450331125827816
max episode reward: -5848675.200000001
mean episode reward: -8974712.813245032
min episode reward: -12960280.099999996
total episodes: 4030
distance: 7036998.899999999
2022-06-19 15:47:44,923	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 897471.0x the scale of `vf_clip_param`. This means that it will take more than 897471.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -2968.6
 reward2: -274792.8
 reward2: -84207.0
 reward2: -197700.4
 reward2: -171963.4
 reward2: -174290.0
 reward2: -490747.6
 reward2: -179694.6
 reward2: -45557.6
 reward2: -217100.4
 reward2: -386147.3
 reward2: -403189.0
 reward2: -16632.9
 reward2: -421464.6
 reward2: -313955.6
 reward2: -487.7
 reward2: -348406.0
 reward2: -371363.8
 reward2: -110244.5
 reward2: -604886.7
 reward2: -224216.6
 reward2: -166002.7
 reward2: -223101.4
 reward2: -92128.5
mean episode length: 26.025974025974026
max episode reward: -4995897.600000001
mean episode reward: -8648092.079870129
min episode reward: -15244532.6
total episodes: 4184
distance: 6270022.700000001
2022-06-19 15:47:56,258	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 864809.0x the scale of `vf_clip_param`. This means that it will take more than 864809.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -184388.0
 reward2: -154936.3
 reward2: -275434.0
 reward2: -2968.6
 reward2: -177787.5
 reward2: -17192.2
 reward2: -530094.3
 reward2: -145495.2
 reward2: -389605.8
 reward2: -722180.2
 reward2: -176588.4
 reward2: -176064.6
 reward2: -620437.4
 reward2: -15096.1
 reward2: -604886.7
 reward2: -224216.6
 reward2: -166002.7
 reward2: -223101.4
 reward2: -189443.4
 reward2: -213945.2
 reward2: -3742.2
 reward2: -490747.6
 reward2: -452754.9
 reward2: -217141.9
 reward2: -53841.1
mean episode length: 26.20394736842105
max episode reward: -5798440.300000002
mean episode reward: -8732984.883552631
min episode reward: -14754290.499999996
total episodes: 4336
distance: 6748672.200000001
2022-06-19 15:48:07,854	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 873298.0x the scale of `vf_clip_param`. This means that it will take more than 873298.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -184388.0
 reward2: -327372.0
 reward2: -166037.4
 reward2: -244473.9
 reward2: -183433.9
 reward2: -104729.1
 reward2: -229690.5
 reward2: -176100.7
 reward2: -117076.7
 reward2: -224216.6
 reward2: -166002.7
 reward2: -193296.6
 reward2: -577371.8
 reward2: -214759.9
 reward2: -15096.1
 reward2: -354191.8
 reward2: -3742.2
 reward2: -490747.6
 reward2: -5528.0
 reward2: -456119.3
 reward2: -313955.6
 reward2: -277386.3
 reward2: -359295.0
 reward2: -84207.0
 reward2: -273596.1
mean episode length: 26.006493506493506
max episode reward: -5342769.7
mean episode reward: -8274829.083116882
min episode reward: -11735023.1
total episodes: 4490
distance: 5818878.5
2022-06-19 15:48:19,675	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 827483.0x the scale of `vf_clip_param`. This means that it will take more than 827483.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -266701.4
 reward2: -110244.5
 reward2: -356807.5
 reward2: -373527.6
 reward2: -166002.7
 reward2: -58407.9
 reward2: -117026.3
 reward2: -617171.4
 reward2: -420845.1
 reward2: -104729.1
 reward2: -229690.5
 reward2: -348406.0
 reward2: -491501.0
 reward2: -45557.6
 reward2: -152341.9
 reward2: -3030.2
 reward2: -166362.4
 reward2: -453956.8
 reward2: -171963.4
 reward2: -112200.2
 reward2: -436733.0
 reward2: -16632.9
 reward2: -722180.2
 reward2: -548016.8
 reward2: -84207.0
mean episode length: 26.07792207792208
max episode reward: -5230767.999999999
mean episode reward: -8293646.970129871
min episode reward: -13847125.899999997
total episodes: 4644
distance: 6894823.300000002
2022-06-19 15:48:31,687	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 829365.0x the scale of `vf_clip_param`. This means that it will take more than 829365.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -2968.6
 reward2: -177787.5
 reward2: -735437.4
 reward2: -84755.5
 reward2: -224216.6
 reward2: -166002.7
 reward2: -271985.0
 reward2: -71589.3
 reward2: -577371.8
 reward2: -457364.3
 reward2: -3742.2
 reward2: -62501.4
 reward2: -217141.9
 reward2: -127597.2
 reward2: -327372.0
 reward2: -718772.7
 reward2: -334217.0
 reward2: -104729.1
 reward2: -92128.5
 reward2: -326058.3
 reward2: -84207.0
 reward2: -137932.5
 reward2: -15096.1
 reward2: -134679.1
mean episode length: 25.935064935064936
max episode reward: -5202100.000000001
mean episode reward: -7918145.892857143
min episode reward: -13940227.299999997
total episodes: 4798
distance: 5657154.899999999
2022-06-19 15:48:43,030	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 791815.0x the scale of `vf_clip_param`. This means that it will take more than 791815.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -63969.4
 reward2: -166037.4
 reward2: -3030.2
 reward2: -178405.7
 reward2: -16632.9
 reward2: -722180.2
 reward2: -487.7
 reward2: -176100.7
 reward2: -139336.7
 reward2: -431877.5
 reward2: -15096.1
 reward2: -318617.8
 reward2: -311275.2
 reward2: -166002.7
 reward2: -58407.9
 reward2: -334896.9
 reward2: -317656.9
 reward2: -197700.4
 reward2: -67168.3
 reward2: -104729.1
 reward2: -289312.1
 reward2: -127597.2
 reward2: -179853.6
 reward2: -491270.3
 reward2: -283543.4
mean episode length: 25.87012987012987
max episode reward: -5085970.199999999
mean episode reward: -7680980.676623375
min episode reward: -10539322.5
total episodes: 4952
distance: 5397559.3
2022-06-19 15:48:54,459	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 768098.0x the scale of `vf_clip_param`. This means that it will take more than 768098.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -63969.4
 reward2: -166037.4
 reward2: -244473.9
 reward2: -183433.9
 reward2: -104729.1
 reward2: -229690.5
 reward2: -176100.7
 reward2: -117076.7
 reward2: -224216.6
 reward2: -166002.7
 reward2: -142959.7
 reward2: -348893.7
 reward2: -213945.2
 reward2: -3742.2
 reward2: -174538.7
 reward2: -449230.1
 reward2: -45557.6
 reward2: -149717.8
 reward2: -5528.0
 reward2: -178405.7
 reward2: -435209.4
 reward2: -163434.9
 reward2: -84207.0
 reward2: -175806.0
 reward2: -199721.1
mean episode length: 25.90967741935484
max episode reward: -4468750.100000001
mean episode reward: -7612631.466451614
min episode reward: -10612517.6
total episodes: 5107
distance: 4585841.5
2022-06-19 15:49:06,073	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 761263.0x the scale of `vf_clip_param`. This means that it will take more than 761263.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -2968.6
 reward2: -177787.5
 reward2: -175251.1
 reward2: -452754.9
 reward2: -217141.9
 reward2: -183433.9
 reward2: -104729.1
 reward2: -229690.5
 reward2: -348406.0
 reward2: -333722.1
 reward2: -224216.6
 reward2: -166002.7
 reward2: -142959.7
 reward2: -176588.4
 reward2: -326058.3
 reward2: -197700.4
 reward2: -3742.2
 reward2: -174538.7
 reward2: -449230.1
 reward2: -45557.6
 reward2: -217100.4
 reward2: -202318.4
 reward2: -137583.6
 reward2: -175806.0
mean episode length: 25.716129032258063
max episode reward: -5064783.2
mean episode reward: -7443803.15483871
min episode reward: -10725406.499999998
total episodes: 5262
distance: 5029350.300000001
2022-06-19 15:49:17,749	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 744380.0x the scale of `vf_clip_param`. This means that it will take more than 744380.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
2022-06-19 15:49:17,776	WARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!
[34m[1mwandb[39m[22m: Adding directory to artifact (./agents/ppo_last_checkpoint/checkpoint_000031)... Done. 0.0s