
2022-05-21 20:08:01,751	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3498645.0x the scale of `vf_clip_param`. This means that it will take more than 3498645.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 51.15384615384615
max episode reward: -9143775.4
mean episode reward: -34986445.83076925
min episode reward: -95673196.09999986
total episodes: 78
2022-05-21 20:08:11,169	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3178899.0x the scale of `vf_clip_param`. This means that it will take more than 3178899.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 47.17
max episode reward: -12179090.600000001
mean episode reward: -31788990.735000018
min episode reward: -83234161.39999998
total episodes: 163
2022-05-21 20:08:21,464	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3076116.0x the scale of `vf_clip_param`. This means that it will take more than 3076116.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 45.98
max episode reward: -10450496.099999998
mean episode reward: -30761157.474000014
min episode reward: -95126726.39999989
total episodes: 250
2022-05-21 20:08:31,431	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2614638.0x the scale of `vf_clip_param`. This means that it will take more than 2614638.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 39.98
max episode reward: -9996892.299999999
mean episode reward: -26146378.02200002
min episode reward: -64439656.50000008
total episodes: 350
2022-05-21 20:08:40,958	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2337281.0x the scale of `vf_clip_param`. This means that it will take more than 2337281.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 36.62385321100918
max episode reward: -10471071.999999996
mean episode reward: -23372814.524770655
min episode reward: -49298169.50000006
total episodes: 459
2022-05-21 20:08:50,301	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1969999.0x the scale of `vf_clip_param`. This means that it will take more than 1969999.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 31.968
max episode reward: -7747050.1
mean episode reward: -19699986.436
min episode reward: -44314132.30000004
total episodes: 584
mean episode length: 30.424242424242426
max episode reward: -6801088.0
mean episode reward: -18415128.025757577
min episode reward: -54423585.10000006
total episodes: 716
2022-05-21 20:08:59,614	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1841513.0x the scale of `vf_clip_param`. This means that it will take more than 1841513.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
2022-05-21 20:09:08,976	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1575105.0x the scale of `vf_clip_param`. This means that it will take more than 1575105.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 27.30821917808219
max episode reward: -6439662.7
mean episode reward: -15751046.297945205
min episode reward: -40349583.70000003
total episodes: 862
mean episode length: 24.857142857142858
max episode reward: -6159448.399999999
mean episode reward: -13835858.950310558
min episode reward: -32633840.700000025
total episodes: 1023
2022-05-21 20:09:18,207	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1383586.0x the scale of `vf_clip_param`. This means that it will take more than 1383586.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
2022-05-21 20:09:27,504	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1184917.0x the scale of `vf_clip_param`. This means that it will take more than 1184917.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 22.40223463687151
max episode reward: -6364613.2
mean episode reward: -11849167.797206704
min episode reward: -27171657.10000001
total episodes: 1202
mean episode length: 20.41025641025641
max episode reward: -5929710.4
mean episode reward: -10298925.164615383
min episode reward: -18615988.9
total episodes: 1397
2022-05-21 20:09:36,983	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1029893.0x the scale of `vf_clip_param`. This means that it will take more than 1029893.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 19.0
max episode reward: -4812735.7
mean episode reward: -9062814.729383884
min episode reward: -20546632.1
total episodes: 1608
2022-05-21 20:09:46,073	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 906281.0x the scale of `vf_clip_param`. This means that it will take more than 906281.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 17.95067264573991
max episode reward: -3918033.4000000004
mean episode reward: -8059559.031390134
min episode reward: -14668060.999999996
total episodes: 1831
2022-05-21 20:09:55,523	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 805956.0x the scale of `vf_clip_param`. This means that it will take more than 805956.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 16.953389830508474
max episode reward: -3427104.2
mean episode reward: -7049219.866949152
min episode reward: -12344847.299999999
total episodes: 2067
2022-05-21 20:10:04,667	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 704922.0x the scale of `vf_clip_param`. This means that it will take more than 704922.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 16.314285714285713
max episode reward: -3256705.3000000003
mean episode reward: -6280426.991836735
min episode reward: -11186139.299999999
total episodes: 2312
2022-05-21 20:10:13,835	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 628043.0x the scale of `vf_clip_param`. This means that it will take more than 628043.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 16.028
max episode reward: -3144555.8
mean episode reward: -5840780.681600001
min episode reward: -10708565.899999999
total episodes: 2562
2022-05-21 20:10:23,402	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 584078.0x the scale of `vf_clip_param`. This means that it will take more than 584078.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.790513833992096
max episode reward: -3089351.8
mean episode reward: -5637963.310671937
min episode reward: -9406343.799999999
total episodes: 2815
2022-05-21 20:10:32,695	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 563796.0x the scale of `vf_clip_param`. This means that it will take more than 563796.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.599221789883268
max episode reward: -3125458.2
mean episode reward: -5235422.949805447
min episode reward: -9119996.799999999
total episodes: 3072
2022-05-21 20:10:41,829	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 523542.0x the scale of `vf_clip_param`. This means that it will take more than 523542.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.5859375
max episode reward: -3097318.6
mean episode reward: -4997983.196484375
min episode reward: -9780822.799999997
total episodes: 3328
2022-05-21 20:10:51,133	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 499798.0x the scale of `vf_clip_param`. This means that it will take more than 499798.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.337164750957854
max episode reward: -2804430.9
mean episode reward: -4474155.947126436
min episode reward: -7322627.8999999985
total episodes: 3589
2022-05-21 20:11:00,520	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 447416.0x the scale of `vf_clip_param`. This means that it will take more than 447416.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.329501915708812
max episode reward: -2603136.8
mean episode reward: -4311371.434482758
min episode reward: -8299445.299999999
total episodes: 3850
2022-05-21 20:11:09,001	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 431137.0x the scale of `vf_clip_param`. This means that it will take more than 431137.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.31800766283525
max episode reward: -2288497.3
mean episode reward: -4071556.7475095787
min episode reward: -6843368.9
total episodes: 4111
2022-05-21 20:11:17,644	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 407156.0x the scale of `vf_clip_param`. This means that it will take more than 407156.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.274809160305344
max episode reward: -2400930.7
mean episode reward: -3863902.99389313
min episode reward: -6356354.6
total episodes: 4373
2022-05-21 20:11:26,088	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 386390.0x the scale of `vf_clip_param`. This means that it will take more than 386390.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.224334600760455
max episode reward: -2181673.1
mean episode reward: -3643041.5368821295
min episode reward: -6019923.9
total episodes: 4636
2022-05-21 20:11:34,734	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 364304.0x the scale of `vf_clip_param`. This means that it will take more than 364304.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.163498098859316
max episode reward: -2089363.5
mean episode reward: -3487437.641444867
min episode reward: -6307398.3
total episodes: 4899
2022-05-21 20:11:43,421	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 348744.0x the scale of `vf_clip_param`. This means that it will take more than 348744.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.132075471698114
max episode reward: -2094502.2
mean episode reward: -3314192.935471698
min episode reward: -5404904.3
total episodes: 5164
2022-05-21 20:11:51,852	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 331419.0x the scale of `vf_clip_param`. This means that it will take more than 331419.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.136363636363637
max episode reward: -2113901.8000000003
mean episode reward: -3199431.039772727
min episode reward: -5840692.5
total episodes: 5428
2022-05-21 20:12:00,275	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 319943.0x the scale of `vf_clip_param`. This means that it will take more than 319943.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.086792452830188
max episode reward: -2086775.2999999998
mean episode reward: -3031170.838867924
min episode reward: -5310914.1
total episodes: 5693
2022-05-21 20:12:08,996	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 303117.0x the scale of `vf_clip_param`. This means that it will take more than 303117.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.132075471698114
max episode reward: -2086873.4
mean episode reward: -2871115.038113208
min episode reward: -5772504.2
total episodes: 5958
2022-05-21 20:12:17,678	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 287112.0x the scale of `vf_clip_param`. This means that it will take more than 287112.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.117424242424242
max episode reward: -2083526.0
mean episode reward: -2908518.8856060603
min episode reward: -5308447.100000001
total episodes: 6222
2022-05-21 20:12:26,413	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 290852.0x the scale of `vf_clip_param`. This means that it will take more than 290852.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 15.09433962264151
max episode reward: -2095013.6
mean episode reward: -2852400.8803773588
min episode reward: -5246154.5
total episodes: 6487
2022-05-21 20:12:35,321	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 285240.0x the scale of `vf_clip_param`. This means that it will take more than 285240.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
2022-05-21 20:12:35,331	WARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!
Traceback (most recent call last):
  File "/home/kartushov/pet_projects/RLTPS/main.py", line 128, in <module>
    agent = train_func(agent)
  File "/home/kartushov/pet_projects/RLTPS/main.py", line 66, in train_func
    artifact = wandb.Artifact(name='PPO Last Checkpoint', type='model')
  File "/home/kartushov/pet_projects/RLTPS/venv/lib/python3.9/site-packages/wandb/sdk/wandb_artifacts.py", line 136, in __init__
    raise ValueError(
ValueError: Artifact name may only contain alphanumeric characters, dashes, underscores, and dots. Invalid name: "PPO Last Checkpoint"
Traceback (most recent call last):
  File "/home/kartushov/pet_projects/RLTPS/main.py", line 128, in <module>
    agent = train_func(agent)
  File "/home/kartushov/pet_projects/RLTPS/main.py", line 66, in train_func
    artifact = wandb.Artifact(name='PPO Last Checkpoint', type='model')
  File "/home/kartushov/pet_projects/RLTPS/venv/lib/python3.9/site-packages/wandb/sdk/wandb_artifacts.py", line 136, in __init__
    raise ValueError(
ValueError: Artifact name may only contain alphanumeric characters, dashes, underscores, and dots. Invalid name: "PPO Last Checkpoint"