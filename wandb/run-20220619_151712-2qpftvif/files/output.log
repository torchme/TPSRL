
2022-06-19 15:17:26,204	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4964040.0x the scale of `vf_clip_param`. This means that it will take more than 4964040.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -782702.2
 reward2: -348893.7
 reward2: -347355.4
 reward2: -176100.7
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 73.37037037037037
max episode reward: -15708046.599999996
mean episode reward: -49640399.80370373
min episode reward: -134146989.2999996
total episodes: 54
distance: 1865896.0999999999
2022-06-19 15:17:37,324	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 5156508.0x the scale of `vf_clip_param`. This means that it will take more than 5156508.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -782702.2
 reward2: -351840.3
 reward2: -370903.3
 reward2: -335644.7
 reward2: -313468.0
 reward2: -228015.1
 reward2: -235605.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 76.01
max episode reward: -12188542.899999999
mean episode reward: -51565075.529000014
min episode reward: -134146989.2999996
total episodes: 106
distance: 3080207.3000000003
2022-06-19 15:17:47,852	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 5088859.0x the scale of `vf_clip_param`. This means that it will take more than 5088859.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -320483.9
 reward2: -462028.6
 reward2: -348406.0
 reward2: -474148.3
 reward2: -15096.1
 reward2: -430655.0
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 75.14
max episode reward: -12188542.899999999
mean episode reward: -50888592.32300002
min episode reward: -112588797.49999975
total episodes: 161
distance: 2122407.2
 reward2: -644652.6
 reward2: -177157.2
 reward2: -457172.1
 reward2: -756216.9
 reward2: -487.7
 reward2: -228015.1
 reward2: -123337.6
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 71.01
max episode reward: -22598593.600000005
mean episode reward: -47719790.44800003
min episode reward: -115613297.39999974
total episodes: 217
distance: 2511754.4
2022-06-19 15:17:58,837	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4771979.0x the scale of `vf_clip_param`. This means that it will take more than 4771979.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -644652.6
 reward2: -177157.2
 reward2: -457172.1
 reward2: -491401.0
 reward2: -124715.2
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 66.71
max episode reward: -17762720.699999996
mean episode reward: -44236244.25200005
min episode reward: -115613297.39999974
total episodes: 280
distance: 1895098.0999999999
2022-06-19 15:18:09,786	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4423624.0x the scale of `vf_clip_param`. This means that it will take more than 4423624.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -107590.5
 reward2: -369721.2
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 67.38
max episode reward: -17762720.699999996
mean episode reward: -44559480.01000002
min episode reward: -125226352.69999966
total episodes: 336
distance: 1002927.7
2022-06-19 15:18:20,957	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4455948.0x the scale of `vf_clip_param`. This means that it will take more than 4455948.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -15096.1
 reward2: -472669.6
 reward2: -371363.8
 reward2: -327380.9
 reward2: -112200.2
 reward2: -313468.0
 reward2: -353968.3
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 66.61
max episode reward: -14773362.899999997
mean episode reward: -44082593.53800003
min episode reward: -125226352.69999966
total episodes: 401
distance: 2475098.1
2022-06-19 15:18:31,893	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4408259.0x the scale of `vf_clip_param`. This means that it will take more than 4408259.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -15096.1
 reward2: -472669.6
 reward2: -371363.8
 reward2: -327380.9
 reward2: -112200.2
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 56.78
max episode reward: -17951470.9
mean episode reward: -36151660.343000025
min episode reward: -90620126.69999991
total episodes: 475
distance: 1565249.1
2022-06-19 15:18:42,429	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3615166.0x the scale of `vf_clip_param`. This means that it will take more than 3615166.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -15096.1
 reward2: -472669.6
 reward2: -371363.8
 reward2: -327380.9
 reward2: -112200.2
 reward2: -457753.6
 reward2: -751396.3
 reward2: -752443.8
 reward2: -494016.6
 reward2: -341065.7
 reward2: -117026.3
 reward2: -176064.6
 reward2: -228015.1
 reward2: -123337.6
 reward2: -370952.4
 reward2: -765427.3
 reward2: -291252.3
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 54.27
max episode reward: -16373545.599999996
mean episode reward: -34090422.560000025
min episode reward: -71964164.60000005
total episodes: 548
distance: 6778791.8999999985
2022-06-19 15:18:53,481	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3409042.0x the scale of `vf_clip_param`. This means that it will take more than 3409042.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -107590.5
 reward2: -327380.9
 reward2: -262310.2
 reward2: -117026.3
 reward2: -177157.2
 reward2: -308516.9
 reward2: -752443.8
 reward2: -3030.2
 reward2: -137884.6
 reward2: -391111.5
 reward2: -123337.6
 reward2: -370952.4
 reward2: -504500.4
 reward2: -464107.9
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 51.6
max episode reward: -11078276.7
mean episode reward: -32118071.76100002
min episode reward: -72371383.40000005
total episodes: 627
distance: 4358392.100000001
2022-06-19 15:19:04,198	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3211807.0x the scale of `vf_clip_param`. This means that it will take more than 3211807.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -107590.5
 reward2: -327380.9
 reward2: -262310.2
 reward2: -117026.3
 reward2: -177157.2
 reward2: -308516.9
 reward2: -752443.8
 reward2: -3030.2
 reward2: -528938.7
 reward2: -123337.6
 reward2: -370903.3
 reward2: -646670.8
 reward2: -765427.3
 reward2: -291252.3
 reward2: -156789.5
 reward2: -286382.3
 reward2: -332625.4
 reward2: -348893.7
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 49.51
max episode reward: -11078276.7
mean episode reward: -30454539.102000013
min episode reward: -82639121.49999997
total episodes: 708
distance: 6403333.500000001
2022-06-19 15:19:15,410	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3045454.0x the scale of `vf_clip_param`. This means that it will take more than 3045454.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -107590.5
 reward2: -369721.2
 reward2: -608770.0
 reward2: -5528.0
 reward2: -528938.7
 reward2: -409449.2
 reward2: -630921.6
 reward2: -752443.8
 reward2: -423591.5
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 47.45
max episode reward: -13839070.499999996
mean episode reward: -28781947.767000012
min episode reward: -64375123.40000008
total episodes: 794
distance: 4414834.8
2022-06-19 15:19:26,086	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2878195.0x the scale of `vf_clip_param`. This means that it will take more than 2878195.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
2022-06-19 15:19:37,182	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2503699.0x the scale of `vf_clip_param`. This means that it will take more than 2503699.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -107590.5
 reward2: -369721.2
 reward2: -608770.0
 reward2: -564792.1
 reward2: -174290.0
 reward2: -127290.4
 reward2: -529923.9
 reward2: -751396.3
 reward2: -752443.8
 reward2: -182441.0
 reward2: -646670.8
 reward2: -311128.3
 reward2: -33544.1
 reward2: -129890.9
 reward2: -448395.2
 reward2: -117026.3
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 42.86
max episode reward: -10650067.700000001
mean episode reward: -25036991.96400001
min episode reward: -59496301.100000076
total episodes: 889
distance: 6142203.499999999
 reward2: -154252.2
 reward2: -107590.5
 reward2: -369721.2
 reward2: -342404.4
 reward2: -58407.9
 reward2: -117026.3
 reward2: -177157.2
 reward2: -457172.1
 reward2: -3030.2
 reward2: -183059.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 39.97
max episode reward: -10864417.599999998
mean episode reward: -22560509.343000002
min episode reward: -55060397.50000006
total episodes: 989
distance: 2600742.6999999997
2022-06-19 15:19:47,991	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2256051.0x the scale of `vf_clip_param`. This means that it will take more than 2256051.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -107590.5
 reward2: -369721.2
 reward2: -608770.0
 reward2: -420845.1
 reward2: -342935.9
 reward2: -224410.6
 reward2: -117026.3
 reward2: -132636.5
 reward2: -349054.7
 reward2: -174538.7
 reward2: -112200.2
 reward2: -457172.1
 reward2: -3030.2
 reward2: -183059.1
 reward2: -635872.7
 reward2: -487.7
 reward2: -351840.3
 reward2: -124715.2
 reward2: -235605.1
 reward2: -156789.5
mean episode length: 37.45794392523364
max episode reward: -9702730.599999998
mean episode reward: -20559194.65327103
min episode reward: -49726492.40000005
total episodes: 1096
distance: 5401767.300000001
2022-06-19 15:19:58,969	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2055919.0x the scale of `vf_clip_param`. This means that it will take more than 2055919.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -291252.3
 reward2: -54005.0
 reward2: -288163.0
 reward2: -529923.9
 reward2: -183059.1
 reward2: -303735.0
 reward2: -286708.1
 reward2: -152341.9
 reward2: -750778.1
 reward2: -58407.9
 reward2: -340459.2
 reward2: -216879.0
 reward2: -71493.8
 reward2: -228237.8
 reward2: -311128.3
 reward2: -319926.4
 reward2: -480958.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 35.67857142857143
max episode reward: -9834479.0
mean episode reward: -19250335.241071433
min episode reward: -36751981.50000002
total episodes: 1208
distance: 5099320.6
2022-06-19 15:20:09,976	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1925034.0x the scale of `vf_clip_param`. This means that it will take more than 1925034.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -286382.3
 reward2: -421004.1
 reward2: -753470.5
 reward2: -228502.8
 reward2: -235605.1
 reward2: -54005.0
 reward2: -327380.9
 reward2: -228237.8
 reward2: -156699.0
 reward2: -209793.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -623955.2
 reward2: -635872.7
 reward2: -620437.4
 reward2: -152341.9
 reward2: -494016.6
 reward2: -3701.8
 reward2: -59926.2
mean episode length: 32.3089430894309
max episode reward: -8547365.799999999
mean episode reward: -16406226.326829268
min episode reward: -37374140.20000003
total episodes: 1331
distance: 6406411.399999999
2022-06-19 15:20:21,318	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1640623.0x the scale of `vf_clip_param`. This means that it will take more than 1640623.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -286382.3
 reward2: -332625.4
 reward2: -348893.7
 reward2: -71493.8
 reward2: -449230.1
 reward2: -635872.7
 reward2: -517327.2
 reward2: -251243.4
 reward2: -370952.4
 reward2: -765427.3
 reward2: -525574.3
 reward2: -92128.5
 reward2: -482875.5
 reward2: -152341.9
 reward2: -494016.6
 reward2: -62501.4
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 31.590551181102363
max episode reward: -7802315.8999999985
mean episode reward: -15904016.063779527
min episode reward: -32415505.600000024
total episodes: 1458
distance: 6533752.0
2022-06-19 15:20:32,146	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1590402.0x the scale of `vf_clip_param`. This means that it will take more than 1590402.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -134679.1
 reward2: -420845.1
 reward2: -104729.1
 reward2: -235605.1
 reward2: -54005.0
 reward2: -247490.3
 reward2: -494016.6
 reward2: -174538.7
 reward2: -228237.8
 reward2: -156699.0
 reward2: -474474.1
 reward2: -318617.8
 reward2: -308516.9
 reward2: -58407.9
 reward2: -117026.3
 reward2: -176064.6
 reward2: -487.7
 reward2: -351840.3
 reward2: -370903.3
mean episode length: 30.28030303030303
max episode reward: -8169587.099999999
mean episode reward: -14747295.655303031
min episode reward: -24780502.30000001
total episodes: 1590
distance: 4851890.1
2022-06-19 15:20:43,438	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1474730.0x the scale of `vf_clip_param`. This means that it will take more than 1474730.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -134679.1
 reward2: -420845.1
 reward2: -104729.1
 reward2: -235605.1
 reward2: -200316.0
 reward2: -355186.7
 reward2: -637952.0
 reward2: -635872.7
 reward2: -505054.6
 reward2: -370602.4
 reward2: -214303.8
 reward2: -474474.1
 reward2: -318617.8
 reward2: -457172.1
 reward2: -750778.1
 reward2: -58407.9
 reward2: -117026.3
 reward2: -379765.3
 reward2: -327380.9
mean episode length: 28.47517730496454
max episode reward: -7071356.499999999
mean episode reward: -13333639.58297872
min episode reward: -23989982.900000006
total episodes: 1731
distance: 7534754.399999999
2022-06-19 15:20:54,030	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1333364.0x the scale of `vf_clip_param`. This means that it will take more than 1333364.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -286382.3
 reward2: -421004.1
 reward2: -179694.6
 reward2: -408464.1
 reward2: -230178.1
 reward2: -487.7
 reward2: -351352.7
 reward2: -214303.8
 reward2: -474474.1
 reward2: -107590.5
 reward2: -327380.9
 reward2: -228237.8
 reward2: -311128.3
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
 reward1: -794596.1
mean episode length: 27.438356164383563
max episode reward: -7406855.399999999
mean episode reward: -12386047.476027396
min episode reward: -27209648.200000014
total episodes: 1877
distance: 4235544.499999999
2022-06-19 15:21:04,880	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1238605.0x the scale of `vf_clip_param`. This means that it will take more than 1238605.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -286382.3
 reward2: -421004.1
 reward2: -179694.6
 reward2: -630921.6
 reward2: -349054.7
 reward2: -199634.4
 reward2: -54005.0
 reward2: -247490.3
 reward2: -755729.2
 reward2: -351352.7
 reward2: -214303.8
 reward2: -474474.1
 reward2: -391437.2
 reward2: -346092.0
 reward2: -311128.3
 reward2: -299834.6
 reward2: -117026.3
 reward2: -176552.3
 reward2: -277386.3
mean episode length: 26.29605263157895
max episode reward: -6124648.500000001
mean episode reward: -11559401.799342103
min episode reward: -18853205.0
total episodes: 2029
distance: 6919489.099999999
2022-06-19 15:21:15,770	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1155940.0x the scale of `vf_clip_param`. This means that it will take more than 1155940.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -286382.3
 reward2: -421004.1
 reward2: -179694.6
 reward2: -630921.6
 reward2: -349054.7
 reward2: -199634.4
 reward2: -161800.6
 reward2: -457172.1
 reward2: -528320.6
 reward2: -230178.1
 reward2: -620925.1
 reward2: -107590.5
 reward2: -327380.9
 reward2: -228237.8
 reward2: -224410.6
 reward2: -117026.3
 reward2: -176064.6
 reward2: -351352.7
 reward2: -214303.8
mean episode length: 25.253164556962027
max episode reward: -6209046.199999998
mean episode reward: -10627688.257594936
min episode reward: -17102278.799999993
total episodes: 2187
distance: 6689711.6
2022-06-19 15:21:26,125	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1062769.0x the scale of `vf_clip_param`. This means that it will take more than 1062769.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -286382.3
 reward2: -342935.9
 reward2: -765427.3
 reward2: -564792.1
 reward2: -174290.0
 reward2: -199634.4
 reward2: -161800.6
 reward2: -308516.9
 reward2: -58407.9
 reward2: -214505.9
 reward2: -230178.1
 reward2: -487.7
 reward2: -351352.7
 reward2: -214303.8
 reward2: -491501.0
 reward2: -498310.7
 reward2: -482875.5
 reward2: -152341.9
 reward2: -244473.9
mean episode length: 24.57668711656442
max episode reward: -5847916.8
mean episode reward: -10167224.425153373
min episode reward: -16529104.099999996
total episodes: 2350
distance: 6249665.100000001
2022-06-19 15:21:37,575	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1016722.0x the scale of `vf_clip_param`. This means that it will take more than 1016722.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -134679.1
 reward2: -525574.3
 reward2: -230178.1
 reward2: -334217.0
 reward2: -303735.0
 reward2: -635872.7
 reward2: -351352.7
 reward2: -214303.8
 reward2: -474474.1
 reward2: -152341.9
 reward2: -750778.1
 reward2: -58407.9
 reward2: -224216.6
 reward2: -373218.1
 reward2: -217137.0
 reward2: -177157.2
 reward2: -112286.3
 reward2: -275386.0
 reward2: -54005.0
mean episode length: 23.81547619047619
max episode reward: -5189604.8
mean episode reward: -9390562.227976188
min episode reward: -15582611.499999996
total episodes: 2518
distance: 6106467.399999999
2022-06-19 15:21:48,335	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 939056.0x the scale of `vf_clip_param`. This means that it will take more than 939056.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -134679.1
 reward2: -525574.3
 reward2: -125953.2
 reward2: -493371.6
 reward2: -755729.2
 reward2: -487.7
 reward2: -334217.0
 reward2: -342935.9
 reward2: -370602.4
 reward2: -214303.8
 reward2: -474474.1
 reward2: -107590.5
 reward2: -127597.2
 reward2: -335644.7
 reward2: -112286.3
 reward2: -138381.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -450111.1
mean episode length: 23.261627906976745
max episode reward: -5591891.9
mean episode reward: -8817875.637209302
min episode reward: -13096459.499999998
total episodes: 2690
distance: 5795881.3
2022-06-19 15:21:59,751	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 881788.0x the scale of `vf_clip_param`. This means that it will take more than 881788.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -134679.1
 reward2: -179694.6
 reward2: -303735.0
 reward2: -327186.7
 reward2: -349054.7
 reward2: -199634.4
 reward2: -54005.0
 reward2: -247490.3
 reward2: -528320.6
 reward2: -230178.1
 reward2: -487.7
 reward2: -351352.7
 reward2: -59926.2
 reward2: -112286.3
 reward2: -228237.8
 reward2: -224410.6
 reward2: -117026.3
 reward2: -210844.1
 reward2: -474474.1
mean episode length: 22.942528735632184
max episode reward: -4780956.999999999
mean episode reward: -8274723.301724137
min episode reward: -14591287.999999996
total episodes: 2864
distance: 4727593.9
2022-06-19 15:22:10,384	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 827472.0x the scale of `vf_clip_param`. This means that it will take more than 827472.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -286382.3
 reward2: -421004.1
 reward2: -179694.6
 reward2: -408464.1
 reward2: -230178.1
 reward2: -487.7
 reward2: -505054.6
 reward2: -370602.4
 reward2: -214303.8
 reward2: -216560.8
 reward2: -493371.6
 reward2: -152305.3
 reward2: -318617.8
 reward2: -112286.3
 reward2: -270992.4
 reward2: -58407.9
 reward2: -450111.1
 reward2: -324466.6
 reward2: -379765.3
mean episode length: 22.738636363636363
max episode reward: -5032973.700000001
mean episode reward: -8085016.988068182
min episode reward: -12282649.899999999
total episodes: 3040
distance: 5900203.299999998
2022-06-19 15:22:21,663	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 808502.0x the scale of `vf_clip_param`. This means that it will take more than 808502.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -286382.3
 reward2: -104729.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -224216.6
 reward2: -768051.4
 reward2: -756216.9
 reward2: -487.7
 reward2: -351352.7
 reward2: -214303.8
 reward2: -154471.1
 reward2: -112286.3
 reward2: -174290.0
 reward2: -199634.4
 reward2: -157115.2
 reward2: -107590.5
 reward2: -127597.2
 reward2: -179853.6
mean episode length: 22.35195530726257
max episode reward: -5320950.999999999
mean episode reward: -7786403.744134079
min episode reward: -12796934.499999998
total episodes: 3219
distance: 4541137.5
2022-06-19 15:22:32,169	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 778640.0x the scale of `vf_clip_param`. This means that it will take more than 778640.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -134679.1
 reward2: -179694.6
 reward2: -408464.1
 reward2: -230178.1
 reward2: -487.7
 reward2: -351352.7
 reward2: -3742.2
 reward2: -217137.0
 reward2: -132636.5
 reward2: -58407.9
 reward2: -301210.0
 reward2: -112286.3
 reward2: -228237.8
 reward2: -156699.0
 reward2: -474474.1
 reward2: -152341.9
 reward2: -244473.9
 reward2: -183433.9
 reward2: -129890.9
mean episode length: 22.318435754189945
max episode reward: -5092319.4
mean episode reward: -7667628.07821229
min episode reward: -17463007.099999998
total episodes: 3398
distance: 4260724.999999999
2022-06-19 15:22:43,651	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 766763.0x the scale of `vf_clip_param`. This means that it will take more than 766763.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -154252.2
 reward2: -15096.1
 reward2: -318292.1
 reward2: -112286.3
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -224216.6
 reward2: -504500.4
 reward2: -487.7
 reward2: -351352.7
 reward2: -214303.8
 reward2: -216560.8
 reward2: -199634.4
 reward2: -54005.0
 reward2: -127597.2
 reward2: -303735.0
 reward2: -421004.1
 reward2: -4946.5
 reward2: -3030.2
mean episode length: 22.171270718232044
max episode reward: -4308410.100000001
mean episode reward: -7241659.9569060765
min episode reward: -11660758.399999997
total episodes: 3579
distance: 3727367.0
2022-06-19 15:22:54,837	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 724166.0x the scale of `vf_clip_param`. This means that it will take more than 724166.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -5528.0
 reward2: -137884.6
 reward2: -45231.9
 reward2: -408464.1
 reward2: -230178.1
 reward2: -487.7
 reward2: -351352.7
 reward2: -171963.4
 reward2: -71589.3
 reward2: -216560.8
 reward2: -217137.0
 reward2: -132636.5
 reward2: -512413.5
 reward2: -247490.3
 reward2: -152305.3
 reward2: -286708.1
 reward2: -318504.4
 reward2: -224216.6
 reward2: -474314.1
 reward2: -161800.6
mean episode length: 22.060773480662984
max episode reward: -4291279.9
mean episode reward: -7003872.835911604
min episode reward: -12490413.699999997
total episodes: 3760
distance: 4949710.999999999
2022-06-19 15:23:05,538	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 700387.0x the scale of `vf_clip_param`. This means that it will take more than 700387.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -5528.0
 reward2: -137884.6
 reward2: -45231.9
 reward2: -124617.4
 reward2: -327380.9
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -142959.7
 reward2: -487.7
 reward2: -351352.7
 reward2: -3742.2
 reward2: -341065.7
 reward2: -224216.6
 reward2: -156699.0
 reward2: -187766.0
 reward2: -129890.9
 reward2: -161800.6
 reward2: -457172.1
 reward2: -152305.3
mean episode length: 21.830601092896174
max episode reward: -4054953.3
mean episode reward: -6697332.433333332
min episode reward: -11317018.799999997
total episodes: 3943
distance: 3544805.8
2022-06-19 15:23:17,506	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 669733.0x the scale of `vf_clip_param`. This means that it will take more than 669733.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -5528.0
 reward2: -137884.6
 reward2: -45231.9
 reward2: -124617.4
 reward2: -327380.9
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -142959.7
 reward2: -487.7
 reward2: -351352.7
 reward2: -3742.2
 reward2: -341065.7
 reward2: -224216.6
 reward2: -156699.0
 reward2: -187766.0
 reward2: -129890.9
 reward2: -161800.6
 reward2: -457172.1
 reward2: -152305.3
mean episode length: 21.71891891891892
max episode reward: -3846644.5
mean episode reward: -6456926.1935135145
min episode reward: -10517672.999999998
total episodes: 4128
distance: 3544805.8
2022-06-19 15:23:31,802	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 645693.0x the scale of `vf_clip_param`. This means that it will take more than 645693.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -5528.0
 reward2: -137884.6
 reward2: -107264.8
 reward2: -127597.2
 reward2: -303735.0
 reward2: -143947.0
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -224216.6
 reward2: -504500.4
 reward2: -487.7
 reward2: -351352.7
 reward2: -3742.2
 reward2: -199634.4
 reward2: -161800.6
 reward2: -457172.1
 reward2: -152305.3
 reward2: -472995.4
mean episode length: 21.594594594594593
max episode reward: -3692436.6000000006
mean episode reward: -6096080.524864865
min episode reward: -8133516.199999998
total episodes: 4313
distance: 4490963.0
2022-06-19 15:23:43,475	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 609608.0x the scale of `vf_clip_param`. This means that it will take more than 609608.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -102432.8
 reward2: -137884.6
 reward2: -134679.1
 reward2: -4946.5
 reward2: -423591.5
 reward2: -143947.0
 reward2: -116460.1
 reward2: -346092.0
 reward2: -156699.0
 reward2: -474474.1
 reward2: -107590.5
 reward2: -127597.2
 reward2: -174142.1
 reward2: -200316.0
 reward2: -341065.7
 reward2: -117026.3
 reward2: -132636.5
 reward2: -142959.7
 reward2: -487.7
 reward2: -351352.7
 reward2: -59926.2
mean episode length: 21.572972972972973
max episode reward: -3166645.3000000007
mean episode reward: -6007213.392432432
min episode reward: -8921995.2
total episodes: 4498
distance: 4280024.1000000015
2022-06-19 15:23:54,936	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 600721.0x the scale of `vf_clip_param`. This means that it will take more than 600721.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -5528.0
 reward2: -137884.6
 reward2: -45231.9
 reward2: -124617.4
 reward2: -327380.9
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -142959.7
 reward2: -487.7
 reward2: -351352.7
 reward2: -3742.2
 reward2: -341065.7
 reward2: -224216.6
 reward2: -156699.0
 reward2: -474474.1
 reward2: -152341.9
 reward2: -423591.5
 reward2: -129890.9
 reward2: -161800.6
mean episode length: 21.473118279569892
max episode reward: -3464290.0000000005
mean episode reward: -5792407.088709678
min episode reward: -9683079.7
total episodes: 4684
distance: 4127434.4999999995
2022-06-19 15:24:06,093	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 579241.0x the scale of `vf_clip_param`. This means that it will take more than 579241.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -450076.8
 reward2: -143947.0
 reward2: -71589.3
 reward2: -156859.3
 reward2: -224410.6
 reward2: -214505.9
 reward2: -92128.5
 reward2: -176064.6
 reward2: -487.7
 reward2: -142949.4
 reward2: -346439.0
 reward2: -3742.2
 reward2: -199634.4
 reward2: -161800.6
 reward2: -320252.1
 reward2: -152341.9
 reward2: -3030.2
 reward2: -137884.6
 reward2: -45231.9
 reward2: -124617.4
 reward2: -244866.3
mean episode length: 21.475935828877006
max episode reward: -3561082.5999999996
mean episode reward: -5612186.390374332
min episode reward: -8834132.200000001
total episodes: 4871
distance: 3511928.1
2022-06-19 15:24:17,628	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 561219.0x the scale of `vf_clip_param`. This means that it will take more than 561219.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -5528.0
 reward2: -137884.6
 reward2: -107264.8
 reward2: -127597.2
 reward2: -303735.0
 reward2: -143947.0
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -224216.6
 reward2: -504500.4
 reward2: -487.7
 reward2: -351352.7
 reward2: -3742.2
 reward2: -199634.4
 reward2: -161800.6
 reward2: -154626.7
 reward2: -474474.1
 reward2: -152341.9
mean episode length: 21.43548387096774
max episode reward: -3694933.4000000004
mean episode reward: -5517117.419892472
min episode reward: -8988680.499999998
total episodes: 5057
distance: 3653808.9
2022-06-19 15:24:29,276	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 551712.0x the scale of `vf_clip_param`. This means that it will take more than 551712.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -104729.1
 reward2: -92128.5
 reward2: -176064.6
 reward2: -487.7
 reward2: -142949.4
 reward2: -58407.9
 reward2: -224216.6
 reward2: -370602.4
 reward2: -3742.2
 reward2: -174538.7
 reward2: -71589.3
 reward2: -154471.1
 reward2: -320252.1
 reward2: -45557.6
 reward2: -174142.1
 reward2: -54005.0
mean episode length: 21.433155080213904
max episode reward: -3150569.1000000006
mean episode reward: -5370642.852941177
min episode reward: -9113173.899999999
total episodes: 5244
distance: 2966184.2
2022-06-19 15:24:40,488	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 537064.0x the scale of `vf_clip_param`. This means that it will take more than 537064.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -104729.1
 reward2: -92128.5
 reward2: -176064.6
 reward2: -487.7
 reward2: -142949.4
 reward2: -58407.9
 reward2: -224216.6
 reward2: -370602.4
 reward2: -3742.2
 reward2: -174538.7
 reward2: -71589.3
 reward2: -154471.1
 reward2: -320252.1
 reward2: -45557.6
 reward2: -174142.1
 reward2: -54005.0
mean episode length: 21.422459893048128
max episode reward: -2985151.9
mean episode reward: -5223318.108021391
min episode reward: -8140797.199999999
total episodes: 5431
distance: 2966184.2
2022-06-19 15:24:52,201	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 522332.0x the scale of `vf_clip_param`. This means that it will take more than 522332.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -104729.1
 reward2: -92128.5
 reward2: -176064.6
 reward2: -487.7
 reward2: -142949.4
 reward2: -58407.9
 reward2: -224216.6
 reward2: -370602.4
 reward2: -3742.2
 reward2: -174538.7
 reward2: -71589.3
 reward2: -154471.1
 reward2: -320252.1
 reward2: -45557.6
 reward2: -174142.1
 reward2: -54005.0
mean episode length: 21.32085561497326
max episode reward: -3315179.1
mean episode reward: -5029402.307486631
min episode reward: -7635398.699999999
total episodes: 5618
distance: 2966184.2
2022-06-19 15:25:03,815	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 502940.0x the scale of `vf_clip_param`. This means that it will take more than 502940.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -481986.5
 reward2: -112286.3
 reward2: -116460.1
 reward2: -92128.5
 reward2: -176064.6
 reward2: -487.7
 reward2: -142949.4
 reward2: -58407.9
 reward2: -224216.6
 reward2: -370602.4
 reward2: -3742.2
 reward2: -199634.4
 reward2: -316178.2
 reward2: -474474.1
 reward2: -107590.5
 reward2: -127597.2
 reward2: -303735.0
 reward2: -421004.1
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
mean episode length: 21.39572192513369
max episode reward: -3082774.9000000004
mean episode reward: -4902169.750802139
min episode reward: -8365668.799999998
total episodes: 5805
distance: 4014620.5000000005
2022-06-19 15:25:15,419	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 490217.0x the scale of `vf_clip_param`. This means that it will take more than 490217.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -143947.0
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -85367.4
 reward2: -487.7
 reward2: -505054.6
 reward2: -370602.4
 reward2: -3742.2
 reward2: -216879.0
 reward2: -154471.1
 reward2: -320252.1
 reward2: -45557.6
 reward2: -174142.1
 reward2: -54005.0
mean episode length: 21.3031914893617
max episode reward: -2795585.500000001
mean episode reward: -4776218.603191489
min episode reward: -8535311.5
total episodes: 5993
distance: 3272441.1
2022-06-19 15:25:26,943	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 477622.0x the scale of `vf_clip_param`. This means that it will take more than 477622.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -143947.0
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -85367.4
 reward2: -487.7
 reward2: -505054.6
 reward2: -156699.0
 reward2: -213945.2
 reward2: -3742.2
 reward2: -62501.4
 reward2: -320252.1
 reward2: -45557.6
 reward2: -174142.1
 reward2: -54005.0
mean episode length: 21.26063829787234
max episode reward: -2711385.7
mean episode reward: -4574074.267553193
min episode reward: -8148758.5
total episodes: 6181
distance: 2963634.2
2022-06-19 15:25:38,134	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 457407.0x the scale of `vf_clip_param`. This means that it will take more than 457407.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -107264.8
 reward2: -183433.9
 reward2: -143947.0
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -85367.4
 reward2: -487.7
 reward2: -505054.6
 reward2: -370602.4
 reward2: -3742.2
 reward2: -216879.0
 reward2: -154471.1
 reward2: -337279.0
 reward2: -45557.6
 reward2: -157115.2
mean episode length: 21.211640211640212
max episode reward: -2924747.6
mean episode reward: -4596994.217989419
min episode reward: -7108806.699999999
total episodes: 6370
distance: 3276503.3000000003
2022-06-19 15:25:49,984	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 459699.0x the scale of `vf_clip_param`. This means that it will take more than 459699.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -104729.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -85367.4
 reward2: -487.7
 reward2: -351352.7
 reward2: -3742.2
 reward2: -174538.7
 reward2: -71589.3
 reward2: -156859.3
 reward2: -311128.3
 reward2: -320252.1
 reward2: -45557.6
 reward2: -174142.1
 reward2: -54005.0
mean episode length: 21.185185185185187
max episode reward: -2943164.3999999994
mean episode reward: -4319313.197883598
min episode reward: -6542443.899999999
total episodes: 6559
distance: 2935224.3000000003
2022-06-19 15:26:01,566	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 431931.0x the scale of `vf_clip_param`. This means that it will take more than 431931.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -450076.8
 reward2: -104729.1
 reward2: -92128.5
 reward2: -176064.6
 reward2: -487.7
 reward2: -142949.4
 reward2: -58407.9
 reward2: -224216.6
 reward2: -370602.4
 reward2: -3742.2
 reward2: -174538.7
 reward2: -71589.3
 reward2: -154471.1
 reward2: -320252.1
 reward2: -157115.2
 reward2: -54005.0
 reward2: -127597.2
 reward2: -179853.6
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
mean episode length: 21.23404255319149
max episode reward: -2866936.6
mean episode reward: -4410435.317021278
min episode reward: -6992795.299999999
total episodes: 6747
distance: 3147902.2000000007
2022-06-19 15:26:11,317	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 441044.0x the scale of `vf_clip_param`. This means that it will take more than 441044.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -104729.1
 reward2: -92128.5
 reward2: -176064.6
 reward2: -487.7
 reward2: -142949.4
 reward2: -58407.9
 reward2: -224216.6
 reward2: -370602.4
 reward2: -3742.2
 reward2: -174538.7
 reward2: -71589.3
 reward2: -154471.1
 reward2: -320252.1
 reward2: -45557.6
 reward2: -174142.1
 reward2: -54005.0
mean episode length: 21.28191489361702
max episode reward: -2579697.9
mean episode reward: -4310570.90212766
min episode reward: -6901390.4
total episodes: 6935
distance: 2966184.2
2022-06-19 15:26:21,042	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 431057.0x the scale of `vf_clip_param`. This means that it will take more than 431057.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -104729.1
 reward2: -117936.0
 reward2: -71589.3
 reward2: -156859.3
 reward2: -166002.7
 reward2: -58407.9
 reward2: -117026.3
 reward2: -176552.3
 reward2: -487.7
 reward2: -351352.7
 reward2: -3742.2
 reward2: -62501.4
 reward2: -320252.1
 reward2: -45557.6
 reward2: -174142.1
 reward2: -54005.0
mean episode length: 21.21276595744681
max episode reward: -2507019.3
mean episode reward: -3969816.832978724
min episode reward: -6932374.500000002
total episodes: 7123
distance: 2779443.6
2022-06-19 15:26:30,751	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 396982.0x the scale of `vf_clip_param`. This means that it will take more than 396982.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
 reward2: -99227.2
 reward2: -4946.5
 reward2: -3030.2
 reward2: -137884.6
 reward2: -286382.3
 reward2: -143947.0
 reward2: -116460.1
 reward2: -92128.5
 reward2: -132636.5
 reward2: -58407.9
 reward2: -85367.4
 reward2: -487.7
 reward2: -348406.0
 reward2: -156859.3
 reward2: -370602.4
 reward2: -3742.2
 reward2: -62501.4
 reward2: -337279.0
 reward2: -45557.6
 reward2: -157115.2
 reward2: -54005.0
mean episode length: 21.147368421052633
max episode reward: -2804084.500000001
mean episode reward: -4020342.6199999996
min episode reward: -6072575.599999999
total episodes: 7313
distance: 2963803.1000000006
2022-06-19 15:26:40,907	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 402034.0x the scale of `vf_clip_param`. This means that it will take more than 402034.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
2022-06-19 15:26:40,937	WARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!
[34m[1mwandb[39m[22m: Adding directory to artifact (./agents/ppo_last_checkpoint/checkpoint_000031)... Done. 0.0s