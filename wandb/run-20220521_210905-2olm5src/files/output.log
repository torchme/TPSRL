mean episode length: 97.46341463414635
max episode reward: -23339399.400000002
mean episode reward: -66987442.35853658
min episode reward: -142381426.29999954
total episodes: 41
2022-05-21 21:09:16,490	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6698744.0x the scale of `vf_clip_param`. This means that it will take more than 6698744.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
2022-05-21 21:09:25,214	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6778734.0x the scale of `vf_clip_param`. This means that it will take more than 6778734.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 98.49382716049382
max episode reward: -23339399.400000002
mean episode reward: -67787338.56296295
min episode reward: -144673506.2999995
total episodes: 81
mean episode length: 100.37
max episode reward: -23339399.400000002
mean episode reward: -69194483.03499998
min episode reward: -144673506.2999995
total episodes: 120
2022-05-21 21:09:33,755	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6919448.0x the scale of `vf_clip_param`. This means that it will take more than 6919448.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 98.23
max episode reward: -25359567.80000001
mean episode reward: -67707391.63
min episode reward: -144673506.2999995
total episodes: 163
2022-05-21 21:09:42,316	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6770739.0x the scale of `vf_clip_param`. This means that it will take more than 6770739.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 93.01
max episode reward: -25359567.80000001
mean episode reward: -63681810.66300001
min episode reward: -143400762.59999955
total episodes: 207
2022-05-21 21:09:50,919	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6368181.0x the scale of `vf_clip_param`. This means that it will take more than 6368181.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 91.92
max episode reward: -26449542.20000001
mean episode reward: -62665731.591000006
min episode reward: -143400762.59999955
total episodes: 250
2022-05-21 21:09:59,477	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6266573.0x the scale of `vf_clip_param`. This means that it will take more than 6266573.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 89.14
max episode reward: -26064262.10000001
mean episode reward: -60338049.883
min episode reward: -138526628.19999954
total episodes: 295
2022-05-21 21:10:08,119	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 6033805.0x the scale of `vf_clip_param`. This means that it will take more than 6033805.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 88.52
max episode reward: -26064262.10000001
mean episode reward: -59894232.15100003
min episode reward: -126690226.99999964
total episodes: 339
2022-05-21 21:10:16,926	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 5989423.0x the scale of `vf_clip_param`. This means that it will take more than 5989423.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 82.03
max episode reward: -21961004.200000007
mean episode reward: -54820336.23700002
min episode reward: -135223362.3999996
total episodes: 392
2022-05-21 21:10:25,540	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 5482034.0x the scale of `vf_clip_param`. This means that it will take more than 5482034.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 74.39
max episode reward: -17027355.5
mean episode reward: -48666258.06000002
min episode reward: -112420151.59999974
total episodes: 445
2022-05-21 21:10:34,085	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4866626.0x the scale of `vf_clip_param`. This means that it will take more than 4866626.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 71.2
max episode reward: -17027355.5
mean episode reward: -46155659.20100002
min episode reward: -112420151.59999974
total episodes: 503
2022-05-21 21:10:42,924	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4615566.0x the scale of `vf_clip_param`. This means that it will take more than 4615566.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 67.01
max episode reward: -16067221.899999999
mean episode reward: -42822704.03500004
min episode reward: -92825776.39999992
total episodes: 564
2022-05-21 21:10:51,525	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4282270.0x the scale of `vf_clip_param`. This means that it will take more than 4282270.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 65.12
max episode reward: -16067221.899999999
mean episode reward: -41236804.18900004
min episode reward: -82583708.49999997
total episodes: 626
2022-05-21 21:11:00,264	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4123680.0x the scale of `vf_clip_param`. This means that it will take more than 4123680.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 67.28
max episode reward: -18932757.5
mean episode reward: -43068591.91900004
min episode reward: -76525715.4
total episodes: 685
2022-05-21 21:11:08,933	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 4306859.0x the scale of `vf_clip_param`. This means that it will take more than 4306859.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 62.05
max episode reward: -18932757.5
mean episode reward: -38957890.29200003
min episode reward: -99026889.09999985
total episodes: 750
2022-05-21 21:11:17,633	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3895789.0x the scale of `vf_clip_param`. This means that it will take more than 3895789.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 59.52
max episode reward: -17843490.999999993
mean episode reward: -36791961.76800003
min episode reward: -99026889.09999985
total episodes: 819
2022-05-21 21:11:26,237	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3679196.0x the scale of `vf_clip_param`. This means that it will take more than 3679196.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 57.32
max episode reward: -11460263.7
mean episode reward: -35014474.27600003
min episode reward: -80454781.6
total episodes: 889
2022-05-21 21:11:34,888	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3501447.0x the scale of `vf_clip_param`. This means that it will take more than 3501447.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 55.25
max episode reward: -14124981.5
mean episode reward: -33416319.290000018
min episode reward: -80454781.6
total episodes: 961
2022-05-21 21:11:43,495	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3341632.0x the scale of `vf_clip_param`. This means that it will take more than 3341632.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 51.95
max episode reward: -14124981.5
mean episode reward: -30817768.595000014
min episode reward: -114047045.39999974
total episodes: 1038
2022-05-21 21:11:52,178	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 3081777.0x the scale of `vf_clip_param`. This means that it will take more than 3081777.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 50.39
max episode reward: -16796622.9
mean episode reward: -29610359.77200001
min episode reward: -56448959.70000006
total episodes: 1117
2022-05-21 21:12:00,958	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2961036.0x the scale of `vf_clip_param`. This means that it will take more than 2961036.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 49.33
max episode reward: -14988025.199999996
mean episode reward: -28714903.73400002
min episode reward: -53781459.40000006
total episodes: 1200
2022-05-21 21:12:09,699	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2871490.0x the scale of `vf_clip_param`. This means that it will take more than 2871490.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 46.28
max episode reward: -12330034.999999998
mean episode reward: -26406707.45900001
min episode reward: -56564861.70000006
total episodes: 1286
2022-05-21 21:12:18,333	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2640671.0x the scale of `vf_clip_param`. This means that it will take more than 2640671.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 45.64
max episode reward: -14302268.599999998
mean episode reward: -26057147.623000022
min episode reward: -53829119.300000064
total episodes: 1373
2022-05-21 21:12:26,986	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2605715.0x the scale of `vf_clip_param`. This means that it will take more than 2605715.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 44.34
max episode reward: -11196725.299999999
mean episode reward: -24865392.110000014
min episode reward: -79373165.4
total episodes: 1463
2022-05-21 21:12:35,680	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2486539.0x the scale of `vf_clip_param`. This means that it will take more than 2486539.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 40.08
max episode reward: -11145636.899999999
mean episode reward: -21464403.455000006
min episode reward: -42814902.80000004
total episodes: 1563
2022-05-21 21:12:44,464	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 2146440.0x the scale of `vf_clip_param`. This means that it will take more than 2146440.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 38.17142857142857
max episode reward: -9925109.9
mean episode reward: -19784373.834285717
min episode reward: -33750994.100000024
total episodes: 1668
2022-05-21 21:12:53,397	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1978437.0x the scale of `vf_clip_param`. This means that it will take more than 1978437.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 38.51456310679612
max episode reward: -10005978.3
mean episode reward: -19954471.519417472
min episode reward: -40193699.00000004
total episodes: 1771
2022-05-21 21:13:02,006	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1995447.0x the scale of `vf_clip_param`. This means that it will take more than 1995447.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 35.716814159292035
max episode reward: -8956596.0
mean episode reward: -17887165.31238938
min episode reward: -37489115.90000003
total episodes: 1884
2022-05-21 21:13:11,042	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1788717.0x the scale of `vf_clip_param`. This means that it will take more than 1788717.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 34.69565217391305
max episode reward: -9094878.5
mean episode reward: -16777811.804347824
min episode reward: -37530785.200000025
total episodes: 1999
2022-05-21 21:13:19,765	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1677781.0x the scale of `vf_clip_param`. This means that it will take more than 1677781.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 34.5
max episode reward: -8215189.9
mean episode reward: -16528925.429310342
min episode reward: -31943992.20000002
total episodes: 2115
2022-05-21 21:13:28,590	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1652893.0x the scale of `vf_clip_param`. This means that it will take more than 1652893.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
mean episode length: 32.58536585365854
max episode reward: -9930042.8
mean episode reward: -15127968.369918697
min episode reward: -31345856.300000023
total episodes: 2238
2022-05-21 21:13:37,535	WARNING ppo.py:162 -- The magnitude of your environment rewards are more than 1512797.0x the scale of `vf_clip_param`. This means that it will take more than 1512797.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.
2022-05-21 21:13:37,542	WARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!
[34m[1mwandb[39m[22m: Adding directory to artifact (./agents/ppo_last_checkpoint/checkpoint_000031)... Done. 0.0s